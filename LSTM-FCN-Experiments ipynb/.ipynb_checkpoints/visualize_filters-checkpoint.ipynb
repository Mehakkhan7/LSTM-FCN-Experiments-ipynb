{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, PReLU, Dense, LSTM, CuDNNLSTM, concatenate, Activation\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "from utils.constants import MAX_SEQUENCE_LENGTH_LIST, NB_CLASSES_LIST, TRAIN_FILES\n",
    "from utils.generic_utils import load_dataset_at\n",
    "from utils.keras_utils import visualize_filters\n",
    "from utils.layer_utils import AttentionLSTM\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def generate_lstmfcn(MAX_SEQUENCE_LENGTH, NB_CLASS, NUM_CELLS=8):\n",
    "\n",
    "    ip = Input(shape=(1, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    x = LSTM(NUM_CELLS)(ip)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_attention_lstmfcn(MAX_SEQUENCE_LENGTH, NB_CLASS, NUM_CELLS=8):\n",
    "\n",
    "    ip = Input(shape=(1, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    x = AttentionLSTM(NUM_CELLS)(ip)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # COMMON PARAMETERS\n",
    "    DATASET_ID = 0\n",
    "    num_cells = 8\n",
    "    model = generate_lstmfcn  # Select model to build\n",
    "\n",
    "    # OLD 85 DATASET PARAMETERS\n",
    "    dataset_name = '' # 'cbf'  # set to None to try to find out automatically for new datasets\n",
    "\n",
    "    # NEW 43 DATASET PARAMETERS\n",
    "    model_name = 'lstmfcn'\n",
    "\n",
    "    # Visualizaion params\n",
    "    CONV_ID = 0\n",
    "    FILTER_ID = 0\n",
    "\n",
    "    \"\"\" <<<<< SCRIPT SETUP >>>>> \"\"\"\n",
    "    # Script setup\n",
    "    sequence_length = MAX_SEQUENCE_LENGTH_LIST[DATASET_ID]\n",
    "    nb_classes = NB_CLASSES_LIST[DATASET_ID]\n",
    "    model = model(sequence_length, nb_classes, num_cells)\n",
    "\n",
    "    if DATASET_ID >= 85:\n",
    "        dataset_name = None\n",
    "\n",
    "    if dataset_name is None:\n",
    "        base_weights_dir = '%s_%d_cells_weights/'\n",
    "        dataset_name = TRAIN_FILES[DATASET_ID][8:-6]\n",
    "        weights_dir = base_weights_dir % (model_name, num_cells)\n",
    "\n",
    "        dataset_name = weights_dir + dataset_name\n",
    "\n",
    "    visualize_filters(model, DATASET_ID, dataset_name, conv_id=CONV_ID, filter_id=FILTER_ID, seed=0,\n",
    "                      normalize_timeseries=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
