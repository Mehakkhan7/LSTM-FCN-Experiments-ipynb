{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.legacy import interfaces\n",
    "from keras.layers import Recurrent\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.int_shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "class AttentionLSTM(Recurrent):\n",
    "    \"\"\"Long-Short Term Memory unit - with Attention.\n",
    "\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](keras/activations.md)).\n",
    "            If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use\n",
    "            for the recurrent step\n",
    "            (see [activations](keras/activations.md)).\n",
    "        attention_activation: Activation function to use\n",
    "            for the attention step. If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "            (see [activations](keras/activations.md)).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix,\n",
    "            used for the linear transformation of the recurrent state.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        attention_initializer: Initializer for the `attention_kernel` weights\n",
    "            matrix, used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        use_chrono_initialization: Boolean.\n",
    "            If True, add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        recurrent_regularizer: Regularizer function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        attention_regularizer: Regularizer function applied to\n",
    "            the `attention_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        recurrent_constraint: Constraint function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "        attention_constraint: Constraint function applied to\n",
    "            the `attention_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the inputs.\n",
    "        recurrent_dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the recurrent state.\n",
    "        return_attention: Returns the attention vector instead of\n",
    "            the internal state.\n",
    "\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "        - [Bahdanau, Cho & Bengio (2014), \"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "        - [Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel & Bengio (2016), \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\"](http://arxiv.org/pdf/1502.03044.pdf)\n",
    "    \"\"\"\n",
    "    @interfaces.legacy_recurrent_support\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 attention_activation='tanh',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 attention_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 attention_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 attention_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        super(AttentionLSTM, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.attention_activation = activations.get(attention_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.attention_initializer = initializers.get(attention_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.attention_regularizer = regularizers.get(attention_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.attention_constraint = constraints.get(attention_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.return_attention = return_attention\n",
    "        self.state_spec = [InputSpec(shape=(None, self.units)),\n",
    "                           InputSpec(shape=(None, self.units))]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        batch_size = input_shape[0] if self.stateful else None\n",
    "        self.timestep_dim = input_shape[1]\n",
    "        self.input_dim = input_shape[2]\n",
    "        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n",
    "\n",
    "        self.states = [None, None]\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        # add attention kernel\n",
    "        self.attention_kernel = self.add_weight(\n",
    "            shape=(self.input_dim, self.units * 4),\n",
    "            name='attention_kernel',\n",
    "            initializer=self.attention_initializer,\n",
    "            regularizer=self.attention_regularizer,\n",
    "            constraint=self.attention_constraint)\n",
    "\n",
    "        # add attention weights\n",
    "        # weights for attention model\n",
    "        self.attention_weights = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                                 name='attention_W',\n",
    "                                                 initializer=self.attention_initializer,\n",
    "                                                 regularizer=self.attention_regularizer,\n",
    "                                                 constraint=self.attention_constraint)\n",
    "\n",
    "        self.attention_recurrent_weights = self.add_weight(shape=(self.units, self.units),\n",
    "                                                           name='attention_U',\n",
    "                                                           initializer=self.recurrent_initializer,\n",
    "                                                           regularizer=self.recurrent_regularizer,\n",
    "                                                           constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                def bias_initializer(shape, *args, **kwargs):\n",
    "                    return K.concatenate([\n",
    "                        self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                        initializers.Ones()((self.units,), *args, **kwargs),\n",
    "                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                    ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "\n",
    "            self.attention_bias = self.add_weight(shape=(self.units,),\n",
    "                                                  name='attention_b',\n",
    "                                                  initializer=self.bias_initializer,\n",
    "                                                  regularizer=self.bias_regularizer,\n",
    "                                                  constraint=self.bias_constraint)\n",
    "\n",
    "            self.attention_recurrent_bias = self.add_weight(shape=(self.units, 1),\n",
    "                                                            name='attention_v',\n",
    "                                                            initializer=self.bias_initializer,\n",
    "                                                            regularizer=self.bias_regularizer,\n",
    "                                                            constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.attention_bias = None\n",
    "            self.attention_recurrent_bias = None\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.units]\n",
    "        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n",
    "        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_o = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        self.attention_i = self.attention_kernel[:, :self.units]\n",
    "        self.attention_f = self.attention_kernel[:, self.units: self.units * 2]\n",
    "        self.attention_c = self.attention_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.attention_o = self.attention_kernel[:, self.units * 3:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_i = self.bias[:self.units]\n",
    "            self.bias_f = self.bias[self.units: self.units * 2]\n",
    "            self.bias_c = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_o = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_i = None\n",
    "            self.bias_f = None\n",
    "            self.bias_c = None\n",
    "            self.bias_o = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        if self.implementation == 0:\n",
    "            input_shape = K.int_shape(inputs)\n",
    "            input_dim = input_shape[2]\n",
    "            timesteps = input_shape[1]\n",
    "\n",
    "            x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,\n",
    "                                          self.dropout, input_dim, self.units,\n",
    "                                          timesteps, training=training)\n",
    "            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        if self.implementation != 0 and 0 < self.dropout < 1:\n",
    "            input_shape = K.int_shape(inputs)\n",
    "            input_dim = input_shape[-1]\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, int(input_dim)))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            dp_mask = [K.in_train_phase(dropped_inputs,\n",
    "                                        ones,\n",
    "                                        training=training) for _ in range(4)]\n",
    "            constants.append(dp_mask)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "        if 0 < self.recurrent_dropout < 1:\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.recurrent_dropout)\n",
    "            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n",
    "                                            ones,\n",
    "                                            training=training) for _ in range(4)]\n",
    "            constants.append(rec_dp_mask)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "        # append the input as well for use later\n",
    "        constants.append(inputs)\n",
    "        return constants\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        dp_mask = states[2]\n",
    "        rec_dp_mask = states[3]\n",
    "        x_input = states[4]\n",
    "\n",
    "        # alignment model\n",
    "        h_att = K.repeat(h_tm1, self.timestep_dim)\n",
    "        att = _time_distributed_dense(x_input, self.attention_weights, self.attention_bias,\n",
    "                                      output_dim=K.int_shape(self.attention_weights)[1])\n",
    "        attention_ = self.attention_activation(K.dot(h_att, self.attention_recurrent_weights) + att) # energy\n",
    "        attention_ = K.squeeze(K.dot(attention_, self.attention_recurrent_bias), 2) # energy\n",
    "\n",
    "        alpha = K.exp(attention_)\n",
    "\n",
    "        if dp_mask is not None:\n",
    "            alpha *= dp_mask[0]\n",
    "\n",
    "        alpha /= K.sum(alpha, axis=1, keepdims=True)\n",
    "        alpha_r = K.repeat(alpha, self.input_dim)\n",
    "        alpha_r = K.permute_dimensions(alpha_r, (0, 2, 1))\n",
    "\n",
    "        # make context vector (soft attention after Bahdanau et al.)\n",
    "        z_hat = x_input * alpha_r\n",
    "        context_sequence = z_hat\n",
    "        z_hat = K.sum(z_hat, axis=1)\n",
    "\n",
    "        if self.implementation == 2:\n",
    "            z = K.dot(inputs * dp_mask[0], self.kernel)\n",
    "            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n",
    "            z += K.dot(z_hat, self.attention_kernel)\n",
    "\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "\n",
    "            z0 = z[:, :self.units]\n",
    "            z1 = z[:, self.units: 2 * self.units]\n",
    "            z2 = z[:, 2 * self.units: 3 * self.units]\n",
    "            z3 = z[:, 3 * self.units:]\n",
    "\n",
    "            i = self.recurrent_activation(z0)\n",
    "            f = self.recurrent_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.recurrent_activation(z3)\n",
    "        else:\n",
    "            if self.implementation == 0:\n",
    "                x_i = inputs[:, :self.units]\n",
    "                x_f = inputs[:, self.units: 2 * self.units]\n",
    "                x_c = inputs[:, 2 * self.units: 3 * self.units]\n",
    "                x_o = inputs[:, 3 * self.units:]\n",
    "            elif self.implementation == 1:\n",
    "                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n",
    "                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n",
    "                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n",
    "                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n",
    "            else:\n",
    "                raise ValueError('Unknown `implementation` mode.')\n",
    "\n",
    "            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel_i)\n",
    "                                              + K.dot(z_hat, self.attention_i))\n",
    "            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1], self.recurrent_kernel_f)\n",
    "                                          + K.dot(z_hat, self.attention_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2], self.recurrent_kernel_c)\n",
    "                                                + K.dot(z_hat, self.attention_c))\n",
    "            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3], self.recurrent_kernel_o)\n",
    "                                          + K.dot(z_hat, self.attention_o))\n",
    "        h = o * self.activation(c)\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            h._uses_learning_phase = True\n",
    "\n",
    "        if self.return_attention:\n",
    "            return context_sequence, [h, c]\n",
    "        else:\n",
    "            return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n",
    "                  'attention_activation': activations.serialize(self.attention_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'attention_initializer': initializers.serialize(self.attention_initializer),\n",
    "                  'use_chrono_initialization': self.unit_forget_bias,\n",
    "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                  'attention_regularizer': regularizers.serialize(self.attention_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'attention_constraint': constraints.serialize(self.attention_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout,\n",
    "                  'return_attention': self.return_attention}\n",
    "        base_config = super(AttentionLSTM, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
