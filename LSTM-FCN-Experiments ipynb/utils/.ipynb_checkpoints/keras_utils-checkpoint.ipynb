{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Permute\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "\n",
    "from utils.generic_utils import load_dataset_at, calculate_dataset_metrics, cutoff_choice, \\\n",
    "    cutoff_sequence, plot_dataset\n",
    "from utils.constants import MAX_SEQUENCE_LENGTH_LIST, TRAIN_FILES\n",
    "\n",
    "mpl.style.use('seaborn-paper')\n",
    "warnings.simplefilter('ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "\n",
    "def train_model(model: Model, dataset_id, dataset_prefix, epochs=50, batch_size=128, val_subset=None,\n",
    "                cutoff=None, normalize_timeseries=False, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Trains a provided Model, given a dataset id.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras Model.\n",
    "        dataset_id: Integer id representing the dataset index containd in\n",
    "            `utils/constants.py`.\n",
    "        dataset_prefix: Name of the dataset. Used for weight saving.\n",
    "        epochs: Number of epochs to train.\n",
    "        batch_size: Size of each batch for training.\n",
    "        val_subset: Optional integer id to subset the test set. To be used if\n",
    "            the test set evaluation time significantly surpasses training time\n",
    "            per epoch.\n",
    "        cutoff: Optional integer which slices of the first `cutoff` timesteps\n",
    "            from the input signal.\n",
    "        normalize_timeseries: Bool / Integer. Determines whether to normalize\n",
    "            the timeseries.\n",
    "\n",
    "            If False, does not normalize the time series.\n",
    "            If True / int not equal to 2, performs standard sample-wise\n",
    "                z-normalization.\n",
    "            If 2: Performs full dataset z-normalization.\n",
    "        learning_rate: Initial learning rate.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                                      normalize_timeseries=normalize_timeseries)\n",
    "    max_nb_words, sequence_length = calculate_dataset_metrics(X_train)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, X_test = cutoff_sequence(X_train, X_test, choice, dataset_id, sequence_length)\n",
    "\n",
    "    if not is_timeseries:\n",
    "        X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH_LIST[dataset_id], padding='post', truncating='post')\n",
    "        X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH_LIST[dataset_id], padding='post', truncating='post')\n",
    "\n",
    "    classes = np.unique(y_train)\n",
    "    le = LabelEncoder()\n",
    "    y_ind = le.fit_transform(y_train.ravel())\n",
    "    recip_freq = len(y_train) / (len(le.classes_) *\n",
    "                                 np.bincount(y_ind).astype(np.float64))\n",
    "    class_weight = recip_freq[le.transform(classes)]\n",
    "\n",
    "    print(\"Class weights : \", class_weight)\n",
    "\n",
    "    y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
    "    y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
    "\n",
    "    if is_timeseries:\n",
    "        factor = 1. / np.cbrt(2)\n",
    "    else:\n",
    "        factor = 1. / np.sqrt(2)\n",
    "\n",
    "    path_splits = os.path.split(dataset_prefix)\n",
    "    if len(path_splits) > 1:\n",
    "        base_path = os.path.join('weights', *path_splits)\n",
    "\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "\n",
    "        base_path = os.path.join(base_path, path_splits[-1])\n",
    "\n",
    "    else:\n",
    "        all_weights_path = os.path.join('weights', dataset_prefix)\n",
    "\n",
    "        if not os.path.exists(all_weights_path):\n",
    "            os.makedirs(all_weights_path)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\"./weights/%s_weights.h5\" % dataset_prefix, verbose=1,\n",
    "                                       monitor='loss', save_best_only=True, save_weights_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', patience=100, mode='auto',\n",
    "                                  factor=factor, cooldown=0, min_lr=1e-4, verbose=2)\n",
    "\n",
    "    callback_list = [model_checkpoint, reduce_lr]\n",
    "\n",
    "    optm = Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    if val_subset is not None:\n",
    "        X_test = X_test[:val_subset]\n",
    "        y_test = y_test[:val_subset]\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callback_list,\n",
    "              class_weight=class_weight, verbose=2, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "def evaluate_model(model: Model, dataset_id, dataset_prefix, batch_size=128, test_data_subset=None,\n",
    "                   cutoff=None, normalize_timeseries=False):\n",
    "    \"\"\"\n",
    "    Evaluates a given Keras Model on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras Model.\n",
    "        dataset_id: Integer id representing the dataset index containd in\n",
    "            `utils/constants.py`.\n",
    "        dataset_prefix: Name of the dataset. Used for weight saving.\n",
    "        batch_size: Size of each batch for evaluation.\n",
    "        test_data_subset: Optional integer id to subset the test set. To be used if\n",
    "            the test set evaluation time is significantly.\n",
    "        cutoff: Optional integer which slices of the first `cutoff` timesteps\n",
    "            from the input signal.\n",
    "        normalize_timeseries: Bool / Integer. Determines whether to normalize\n",
    "            the timeseries.\n",
    "\n",
    "            If False, does not normalize the time series.\n",
    "            If True / int not equal to 2, performs standard sample-wise\n",
    "                z-normalization.\n",
    "            If 2: Performs full dataset z-normalization.\n",
    "\n",
    "    Returns:\n",
    "        The test set accuracy of the model.\n",
    "    \"\"\"\n",
    "    _, _, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                          normalize_timeseries=normalize_timeseries)\n",
    "    max_nb_words, sequence_length = calculate_dataset_metrics(X_test)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            _, X_test = cutoff_sequence(None, X_test, choice, dataset_id, sequence_length)\n",
    "\n",
    "    if not is_timeseries:\n",
    "        X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH_LIST[dataset_id], padding='post', truncating='post')\n",
    "    y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
    "\n",
    "    optm = Adam(lr=1e-3)\n",
    "    model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "    print(\"Weights loaded from \", \"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    if test_data_subset is not None:\n",
    "        X_test = X_test[:test_data_subset]\n",
    "        y_test = y_test[:test_data_subset]\n",
    "\n",
    "    print(\"\\nEvaluating : \")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print()\n",
    "    print(\"Final Accuracy : \", accuracy)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def loss_model(model: Model, dataset_id, dataset_prefix, batch_size=128, train_data_subset=None,\n",
    "               cutoff=None, normalize_timeseries=False):\n",
    "    X_train, y_train, _, _, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                            normalize_timeseries=normalize_timeseries)\n",
    "\n",
    "    y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
    "\n",
    "    optm = Adam(lr=1e-3)\n",
    "    model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "    print(\"Weights loaded from \", \"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    if train_data_subset is not None:\n",
    "        X_train = X_train[:train_data_subset]\n",
    "        y_train = y_train[:train_data_subset]\n",
    "\n",
    "    print(\"\\nEvaluating : \")\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, batch_size=batch_size)\n",
    "    print()\n",
    "    print(\"Final Loss : \", loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def set_trainable(layer, value):\n",
    "    \"\"\"\n",
    "    Sets the layers of the Model to be trainable or not.\n",
    "\n",
    "    Args:\n",
    "        layer: can be a single Layer of a Model, or an entire Model.\n",
    "        value: True or False.\n",
    "    \"\"\"\n",
    "    layer.trainable = value\n",
    "\n",
    "    # case: container\n",
    "    if hasattr(layer, 'layers'):\n",
    "        for l in layer.layers:\n",
    "            set_trainable(l, value)\n",
    "\n",
    "    # case: wrapper (which is a case not covered by the PR)\n",
    "    if hasattr(layer, 'layer'):\n",
    "        set_trainable(layer.layer, value)\n",
    "\n",
    "\n",
    "def build_function(model, layer_names=None, outputs=None):\n",
    "    \"\"\"\n",
    "    Builds a Keras Function which retrieves the output of a Layer.\n",
    "\n",
    "    Args:\n",
    "        model: Keras Model.\n",
    "        layer_names: Name of the layer whose output is required.\n",
    "        outputs: Output tensors.\n",
    "\n",
    "    Returns:\n",
    "        List of Keras Functions.\n",
    "    \"\"\"\n",
    "    inp = model.input\n",
    "\n",
    "    if layer_names is not None and (type(layer_names) != list and type(layer_names) != tuple):\n",
    "        layer_names = [layer_names]\n",
    "\n",
    "    if outputs is None:\n",
    "        if layer_names is None:\n",
    "            outputs = [layer.output for layer in model.layers]  # all layer outputs\n",
    "        else:\n",
    "            outputs = [layer.output for layer in model.layers if layer.name in layer_names]\n",
    "    else:\n",
    "        outputs = outputs\n",
    "\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    return funcs\n",
    "\n",
    "\n",
    "def get_outputs(model, inputs, eval_functions, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets the outputs of the Keras model.\n",
    "\n",
    "    Args:\n",
    "        model: Unused.\n",
    "        inputs: Input numpy arrays.\n",
    "        eval_functions: Keras functions for evaluation.\n",
    "        verbose: Whether to print evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        List of outputs of the Keras Model.\n",
    "    \"\"\"\n",
    "    if verbose: print('----- activations -----')\n",
    "    outputs = []\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in eval_functions]\n",
    "    for layer_activations in layer_outputs:\n",
    "        outputs.append(layer_activations)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def visualize_context_vector(model: Model, dataset_id, dataset_prefix, cutoff=None, limit=None,\n",
    "                             normalize_timeseries=False, visualize_sequence=True, visualize_classwise=False):\n",
    "    \"\"\"\n",
    "    Visualize the Context Vector of the Attention LSTM.\n",
    "\n",
    "    Args:\n",
    "        model: an Attention LSTM-FCN Model.\n",
    "        dataset_id: Integer id representing the dataset index containd in\n",
    "            `utils/constants.py`.\n",
    "        dataset_prefix: Name of the dataset. Used for weight saving.\n",
    "        batch_size: Size of each batch for evaluation.\n",
    "        test_data_subset: Optional integer id to subset the test set. To be used if\n",
    "            the test set evaluation time is significantly.\n",
    "        cutoff: Optional integer which slices of the first `cutoff` timesteps\n",
    "            from the input signal.\n",
    "        limit: Number of samples to be visualized in one plot.\n",
    "        normalize_timeseries: Bool / Integer. Determines whether to normalize\n",
    "            the timeseries.\n",
    "\n",
    "            If False, does not normalize the time series.\n",
    "            If True / int not equal to 2, performs standard sample-wise\n",
    "                z-normalization.\n",
    "            If 2: Performs full dataset z-normalization.\n",
    "        visualize_sequence: Bool flag, whetehr to visualize the sequence attended to\n",
    "            by the Context Vector or just the Context Vector itself.\n",
    "        visualize_classwise: Bool flag. Wheter to visualize the samples\n",
    "            seperated by class. When doing so, `limit` is multiplied by\n",
    "            the number of classes so it is better to set `limit` to 1 in\n",
    "            such cases.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                                      normalize_timeseries=normalize_timeseries)\n",
    "    _, sequence_length = calculate_dataset_metrics(X_train)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, X_test = cutoff_sequence(X_train, X_test, choice, dataset_id, sequence_length)\n",
    "\n",
    "    attn_lstm_layer = [(i, layer) for (i, layer) in enumerate(model.layers)\n",
    "                       if layer.__class__.__name__ == 'AttentionLSTM']\n",
    "\n",
    "    if len(attn_lstm_layer) == 0:\n",
    "        raise AttributeError('Provided model does not have an Attention layer')\n",
    "    else:\n",
    "        i, attn_lstm_layer = attn_lstm_layer[0]  # use first attention lstm layer only\n",
    "\n",
    "    attn_lstm_layer.return_attention = True\n",
    "\n",
    "    model.layers[i] = attn_lstm_layer\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    attention_output = model.layers[i].call(model.input)\n",
    "\n",
    "    eval_functions = build_function(model, attn_lstm_layer.name, outputs=[attention_output])\n",
    "    train_attention_vectors = []\n",
    "    test_attention_vectors = []\n",
    "\n",
    "    output_shape = [X_train.shape[-1], 1, 1]\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        activations = get_outputs(model,\n",
    "                                  X_train[i, :, :][np.newaxis, ...],\n",
    "                                  eval_functions,\n",
    "                                  verbose=False)[0]\n",
    "\n",
    "        # print(\"activations\", activations.shape)\n",
    "        attention_vector = activations.reshape((-1, 1, 1))\n",
    "\n",
    "        attention_vector = (attention_vector - attention_vector.min()) / (\n",
    "                attention_vector.max() - attention_vector.min())\n",
    "        attention_vector = (attention_vector * 2.) - 1.\n",
    "\n",
    "        attention_vector = resize(attention_vector, output_shape, mode='reflect', anti_aliasing=True)\n",
    "        attention_vector = attention_vector.reshape([1, -1])\n",
    "        train_attention_vectors.append(attention_vector)\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        activations = get_outputs(model,\n",
    "                                  X_test[i, :, :][np.newaxis, ...],\n",
    "                                  eval_functions,\n",
    "                                  verbose=False)[0]\n",
    "\n",
    "        # print(\"activations\", activations.shape)\n",
    "        attention_vector = activations.reshape((-1, 1, 1))\n",
    "\n",
    "        attention_vector = (attention_vector - attention_vector.min()) / (\n",
    "                attention_vector.max() - attention_vector.min())\n",
    "        attention_vector = (attention_vector * 2.) - 1.\n",
    "\n",
    "        attention_vector = resize(attention_vector, output_shape, mode='reflect', anti_aliasing=True)\n",
    "        attention_vector = attention_vector.reshape([1, -1])\n",
    "        test_attention_vectors.append(attention_vector)\n",
    "\n",
    "    train_attention_vectors = np.array(train_attention_vectors)\n",
    "    test_attention_vectors = np.array(test_attention_vectors)\n",
    "\n",
    "    print(\"Train Attention Vectors Shape :\", train_attention_vectors.shape)\n",
    "    print(\"Test Attentin Vectors Shape :\", test_attention_vectors.shape)\n",
    "\n",
    "    if visualize_sequence:\n",
    "        # plot input sequence part that is paid attention too in detail\n",
    "        X_train_attention = train_attention_vectors * X_train\n",
    "        X_test_attention = test_attention_vectors * X_test\n",
    "\n",
    "        plot_dataset(dataset_id, seed=1, limit=limit, cutoff=cutoff,\n",
    "                     normalize_timeseries=normalize_timeseries, plot_data=(X_train, y_train, X_test, y_test,\n",
    "                                                                           X_train_attention, X_test_attention),\n",
    "                     type='Context', plot_classwise=visualize_classwise)\n",
    "\n",
    "    else:\n",
    "        # plot only attention chart\n",
    "        choice = np.random.randint(0, train_attention_vectors.shape[0])\n",
    "\n",
    "        train_df = pd.DataFrame({'attention (%)': train_attention_vectors[choice, 0]},\n",
    "                                index=range(train_attention_vectors.shape[-1]))\n",
    "\n",
    "        train_df.plot(kind='bar',\n",
    "                      title='Attention Mechanism (Train) as '\n",
    "                            'a function of input'\n",
    "                            ' dimensions. Class = %d' % (\n",
    "                                y_train[choice]\n",
    "                            ))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def write_context_vector(model: Model, dataset_id, dataset_prefix, cutoff=None, limit=None,\n",
    "                         normalize_timeseries=False, visualize_sequence=True, visualize_classwise=False):\n",
    "    \"\"\" Same as visualize_context_vector, but writes the context vectors to a file. Unused. \"\"\"\n",
    "\n",
    "    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                                      normalize_timeseries=normalize_timeseries)\n",
    "    _, sequence_length = calculate_dataset_metrics(X_train)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, X_test = cutoff_sequence(X_train, X_test, choice, dataset_id, sequence_length)\n",
    "\n",
    "    attn_lstm_layer = [(i, layer) for (i, layer) in enumerate(model.layers)\n",
    "                       if layer.__class__.__name__ == 'AttentionLSTM']\n",
    "\n",
    "    if len(attn_lstm_layer) == 0:\n",
    "        raise AttributeError('Provided model does not have an Attention layer')\n",
    "    else:\n",
    "        i, attn_lstm_layer = attn_lstm_layer[0]  # use first attention lstm layer only\n",
    "\n",
    "    attn_lstm_layer.return_attention = True\n",
    "\n",
    "    model.layers[i] = attn_lstm_layer\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    attention_output = model.layers[i].call(model.input)\n",
    "\n",
    "    eval_functions = build_function(model, attn_lstm_layer.name, outputs=[attention_output])\n",
    "    train_attention_vectors = []\n",
    "    test_attention_vectors = []\n",
    "\n",
    "    if not os.path.exists('lstm_features/'):\n",
    "        os.makedirs('lstm_features/')\n",
    "\n",
    "    output_shape = [X_train.shape[-1], 1, 1]\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        activations = get_outputs(model,\n",
    "                                  X_train[i, :, :][np.newaxis, ...],\n",
    "                                  eval_functions,\n",
    "                                  verbose=False)[0]\n",
    "\n",
    "        # print(\"activations\", activations.shape)\n",
    "        attention_vector = activations.reshape((-1, 1, 1))\n",
    "\n",
    "        attention_vector = (attention_vector - attention_vector.min()) / (\n",
    "                attention_vector.max() - attention_vector.min())\n",
    "        attention_vector = (attention_vector * 2.) - 1.\n",
    "\n",
    "        attention_vector = resize(attention_vector, output_shape, mode='reflect', anti_aliasing=True)\n",
    "        attention_vector = attention_vector.reshape([1, -1])\n",
    "        train_attention_vectors.append(attention_vector)\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        activations = get_outputs(model,\n",
    "                                  X_test[i, :, :][np.newaxis, ...],\n",
    "                                  eval_functions,\n",
    "                                  verbose=False)[0]\n",
    "\n",
    "        # print(\"activations\", activations.shape)\n",
    "        attention_vector = activations.reshape((-1, 1, 1))\n",
    "\n",
    "        attention_vector = (attention_vector - attention_vector.min()) / (\n",
    "                attention_vector.max() - attention_vector.min())\n",
    "        attention_vector = (attention_vector * 2.) - 1.\n",
    "\n",
    "        attention_vector = resize(attention_vector, output_shape, mode='reflect', anti_aliasing=True)\n",
    "        attention_vector = attention_vector.reshape([1, -1])\n",
    "        test_attention_vectors.append(attention_vector)\n",
    "\n",
    "    train_attention_vectors = np.array(train_attention_vectors)\n",
    "    test_attention_vectors = np.array(test_attention_vectors)\n",
    "\n",
    "    print(\"Train Attention Vectors Shape :\", train_attention_vectors.shape)\n",
    "    print(\"Test Attentin Vectors Shape :\", test_attention_vectors.shape)\n",
    "\n",
    "    if visualize_sequence:\n",
    "        # plot input sequence part that is paid attention too in detail\n",
    "        X_train_attention = train_attention_vectors * X_train\n",
    "        X_test_attention = test_attention_vectors * X_test\n",
    "\n",
    "        X_train_attention = X_train_attention.squeeze(1)\n",
    "        X_test_attention = X_test_attention.squeeze(1)\n",
    "\n",
    "        df = pd.DataFrame(X_test_attention)\n",
    "        df['label'] = y_test[:, 0]\n",
    "\n",
    "        df.to_csv('lstm_features/features.csv')\n",
    "\n",
    "    else:\n",
    "        # plot only attention chart\n",
    "        choice = np.random.randint(0, train_attention_vectors.shape[0])\n",
    "\n",
    "        train_df = pd.DataFrame({'attention (%)': train_attention_vectors[choice, 0]},\n",
    "                                index=range(train_attention_vectors.shape[-1]))\n",
    "\n",
    "        train_df.plot(kind='bar',\n",
    "                      title='Attention Mechanism (Train) as '\n",
    "                            'a function of input'\n",
    "                            ' dimensions. Class = %d' % (\n",
    "                                y_train[choice]\n",
    "                            ))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def visualize_cam(model: Model, dataset_id, dataset_prefix, class_id,\n",
    "                  cutoff=None, normalize_timeseries=False, seed=0):\n",
    "    \"\"\"\n",
    "    Used to visualize the Class Activation Maps of the Keras Model.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras Model.\n",
    "        dataset_id: Integer id representing the dataset index containd in\n",
    "            `utils/constants.py`.\n",
    "        dataset_prefix: Name of the dataset. Used for weight saving.\n",
    "        class_id: Index of the class whose activation is to be visualized.\n",
    "        cutoff: Optional integer which slices of the first `cutoff` timesteps\n",
    "            from the input signal.\n",
    "        normalize_timeseries: Bool / Integer. Determines whether to normalize\n",
    "            the timeseries.\n",
    "\n",
    "            If False, does not normalize the time series.\n",
    "            If True / int not equal to 2, performs standard sample-wise\n",
    "                z-normalization.\n",
    "            If 2: Performs full dataset z-normalization.\n",
    "        seed: Random seed number for Numpy.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_train, y_train, _, _, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                            normalize_timeseries=normalize_timeseries)\n",
    "    _, sequence_length = calculate_dataset_metrics(X_train)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, _ = cutoff_sequence(X_train, _, choice, dataset_id, sequence_length)\n",
    "\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "\n",
    "    conv_layers = [layer for layer in model.layers if layer.__class__.__name__ == 'Conv1D']\n",
    "\n",
    "    final_conv = conv_layers[-1].name\n",
    "    final_softmax = model.layers[-1].name\n",
    "    out_names = [final_conv, final_softmax]\n",
    "\n",
    "    if class_id > 0:\n",
    "        class_id = class_id - 1\n",
    "\n",
    "    y_train_ids = np.where(y_train[:, 0] == class_id)\n",
    "    sequence_input = X_train[y_train_ids[0], ...]\n",
    "    choice = np.random.choice(range(len(sequence_input)), 1)\n",
    "    sequence_input = sequence_input[choice, :, :]\n",
    "\n",
    "    eval_functions = build_function(model, out_names)\n",
    "    conv_out, predictions = get_outputs(model, sequence_input, eval_functions)\n",
    "\n",
    "    conv_out = conv_out[0, :, :]  # (T, C)\n",
    "\n",
    "    conv_out = (conv_out - conv_out.min(axis=0, keepdims=True)) / \\\n",
    "               (conv_out.max(axis=0, keepdims=True) - conv_out.min(axis=0, keepdims=True))\n",
    "    conv_out = (conv_out * 2.) - 1.\n",
    "\n",
    "    conv_out = conv_out.transpose((1, 0))  # (C, T)\n",
    "    conv_channels = conv_out.shape[0]\n",
    "\n",
    "    conv_cam = class_weights[:conv_channels, [class_id]] * conv_out\n",
    "    conv_cam = np.sum(conv_cam, axis=0)\n",
    "\n",
    "    conv_cam /= conv_cam.max()\n",
    "\n",
    "    sequence_input = sequence_input.reshape((-1, 1))\n",
    "    conv_cam = conv_cam.reshape((-1, 1))\n",
    "\n",
    "    sequence_df = pd.DataFrame(sequence_input,\n",
    "                               index=range(sequence_input.shape[0]),\n",
    "                               columns=range(sequence_input.shape[1]))\n",
    "\n",
    "    conv_cam_df = pd.DataFrame(conv_cam,\n",
    "                               index=range(conv_cam.shape[0]),\n",
    "                               columns=[1])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, squeeze=False,\n",
    "                            figsize=(6, 6))\n",
    "\n",
    "    class_label = class_id + 1\n",
    "\n",
    "    sequence_df.plot(title='Sequence (class = %d)' % (class_label),\n",
    "                     subplots=False,\n",
    "                     legend=None,\n",
    "                     ax=axs[0][0])\n",
    "\n",
    "    conv_cam_df.plot(title='Convolution Class Activation Map (class = %d)' % (class_label),\n",
    "                     subplots=False,\n",
    "                     legend=None,\n",
    "                     ax=axs[1][0])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def write_cam(model: Model, dataset_id, dataset_prefix,\n",
    "              cutoff=None, normalize_timeseries=False):\n",
    "    \"\"\" Same as visualize_cam, but writes the result data to a file. \"\"\"\n",
    "\n",
    "    _, _, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                          normalize_timeseries=normalize_timeseries)\n",
    "    _, sequence_length = calculate_dataset_metrics(X_test)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, _ = cutoff_sequence(_, X_test, choice, dataset_id, sequence_length)\n",
    "\n",
    "    print(\"Weights path : \", \"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "\n",
    "    conv_layers = [layer for layer in model.layers if layer.__class__.__name__ == 'Conv1D']\n",
    "\n",
    "    final_conv = conv_layers[-1].name\n",
    "    final_softmax = model.layers[-1].name\n",
    "    out_names = [final_conv, final_softmax]\n",
    "\n",
    "    eval_functions = build_function(model, out_names)\n",
    "\n",
    "    parts = os.path.split(dataset_prefix)\n",
    "    if len(parts) > 1:\n",
    "        basepath = os.path.join('cam_features', os.pathsep.join(parts[:-1]))\n",
    "        dataset_name = parts[-1]\n",
    "    else:\n",
    "        basepath = 'cam_features/'\n",
    "        dataset_name = dataset_prefix\n",
    "\n",
    "    if not os.path.exists(basepath):\n",
    "        os.makedirs(basepath)\n",
    "\n",
    "    cam_features = []\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        print(\"Sample %d running\" % (i + 1))\n",
    "        y_id = y_test[i, 0]\n",
    "        sequence_input = X_test[[i], :, :]\n",
    "\n",
    "        conv_out, predictions = get_outputs(model, sequence_input, eval_functions)\n",
    "\n",
    "        conv_out = conv_out[0, :, :]  # (T, C)\n",
    "\n",
    "        conv_out = conv_out.transpose((1, 0))  # (C, T)\n",
    "        conv_channels = conv_out.shape[0]\n",
    "        conv_cam = class_weights[:conv_channels, [int(y_id)]] * conv_out\n",
    "        conv_cam = np.mean(conv_cam, axis=0)\n",
    "\n",
    "        conv_cam = conv_cam.reshape((-1, 1))\n",
    "        cam_features.append(conv_cam)\n",
    "\n",
    "    cam_features = np.concatenate(cam_features, -1).transpose()\n",
    "    print(\"Num features = \", cam_features.shape)\n",
    "\n",
    "    conv_cam_df = pd.DataFrame(cam_features)\n",
    "\n",
    "    conv_cam_df.to_csv(basepath + '/%s_features_mean_unnormalized.csv' % dataset_name,\n",
    "                       header=False, index=False)\n",
    "\n",
    "\n",
    "def visualize_filters(model: Model, dataset_id, dataset_prefix,\n",
    "                      conv_id=0, filter_id=0, seed=0, cutoff=None,\n",
    "                      normalize_timeseries=False):\n",
    "    \"\"\"\n",
    "    Used to visualize the output filters of a particular convolution layer.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras Model.\n",
    "        dataset_id: Integer id representing the dataset index containd in\n",
    "            `utils/constants.py`.\n",
    "        dataset_prefix: Name of the dataset. Used for weight saving.\n",
    "        conv_id: Convolution layer ID. Can be 0, 1 or 2 for LSTMFCN and\n",
    "            its univariate variants (as it uses 3 Conv blocks).\n",
    "        filter_id: ID of the filter that is under observation.\n",
    "        seed: Numpy random seed.\n",
    "        cutoff: Optional integer which slices of the first `cutoff` timesteps\n",
    "            from the input signal.\n",
    "        normalize_timeseries: Bool / Integer. Determines whether to normalize\n",
    "            the timeseries.\n",
    "\n",
    "            If False, does not normalize the time series.\n",
    "            If True / int not equal to 2, performs standard sample-wise\n",
    "                z-normalization.\n",
    "            If 2: Performs full dataset z-normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    assert conv_id >= 0 and conv_id < 3, \"Convolution layer ID must be between 0 and 2\"\n",
    "\n",
    "    X_train, y_train, _, _, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                            normalize_timeseries=normalize_timeseries)\n",
    "    _, sequence_length = calculate_dataset_metrics(X_train)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, _ = cutoff_sequence(X_train, _, choice, dataset_id, sequence_length)\n",
    "\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    conv_layers = [layer for layer in model.layers\n",
    "                   if layer.__class__.__name__ == 'Conv1D']\n",
    "\n",
    "    conv_layer = conv_layers[conv_id]\n",
    "    conv_layer_name = conv_layer.name\n",
    "\n",
    "    eval_functions = build_function(model, [conv_layer_name])\n",
    "\n",
    "    save_dir = os.path.split(dataset_prefix)[0]\n",
    "\n",
    "    if not os.path.exists('cnn_filters/%s' % (save_dir)):\n",
    "        os.makedirs('cnn_filters/%s' % (save_dir))\n",
    "\n",
    "    dataset_name = os.path.split(dataset_prefix)[-1]\n",
    "    if dataset_name is None or len(dataset_name) == 0:\n",
    "        dataset_name = dataset_prefix\n",
    "\n",
    "    # Select single datapoint\n",
    "    sample_index = np.random.randint(0, X_train.shape[0])\n",
    "    y_id = y_train[sample_index, 0]\n",
    "    sequence_input = X_train[[sample_index], :, :]\n",
    "\n",
    "    # Get features of the cnn layer out\n",
    "    conv_out = get_outputs(model, sequence_input, eval_functions)[0]\n",
    "\n",
    "    conv_out = conv_out[0, :, :]  # [T, C]\n",
    "\n",
    "    # select single filter\n",
    "    assert filter_id > 0 and filter_id < conv_out.shape[-1]\n",
    "    channel = conv_out[:, filter_id]\n",
    "    channel = channel.reshape((-1, 1))\n",
    "\n",
    "    conv_filters = pd.DataFrame(channel)\n",
    "    conv_filters.to_csv('cnn_filters/%s_features.csv' % (dataset_prefix), header=None, index=False)\n",
    "\n",
    "    sequence_input = sequence_input[0, :, :].transpose()\n",
    "    sequence_df = pd.DataFrame(sequence_input,\n",
    "                               index=range(sequence_input.shape[0]))\n",
    "\n",
    "    conv_cam_df = pd.DataFrame(conv_filters,\n",
    "                               index=range(conv_filters.shape[0]))\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, squeeze=False,\n",
    "                            figsize=(6, 6))\n",
    "\n",
    "    class_label = y_id + 1\n",
    "\n",
    "    plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "    sequence_df.plot(title='Dataset %s : Sequence ID = %d (class = %d)' % (dataset_name,\n",
    "                                                                           sample_index + 1,\n",
    "                                                                           class_label),\n",
    "                     subplots=False,\n",
    "                     legend=None,\n",
    "                     ax=axs[0][0])\n",
    "\n",
    "    conv_cam_df.plot(title='Convolution Layer %d Filter ID %d (class = %d)' % (conv_id + 1,\n",
    "                                                                               filter_id + 1,\n",
    "                                                                               class_label),\n",
    "                     subplots=False,\n",
    "                     legend=None,\n",
    "                     ax=axs[1][0])\n",
    "\n",
    "    # Formatting\n",
    "    plt.xlabel('Timesteps', axes=axs[0][0])\n",
    "\n",
    "    axs[0][0].set_ylabel('Value')\n",
    "    axs[1][0].set_ylabel('Value')\n",
    "\n",
    "    def mjrFormatter(x, pos):\n",
    "        return '{:.2f}'.format(x)\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(mjrFormatter))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_features(model: Model, dataset_id, dataset_prefix,\n",
    "                     layer_name, cutoff=None, normalize_timeseries=False):\n",
    "    \"\"\" Same as visualize_features, but saves them to a file instead. \"\"\"\n",
    "\n",
    "    layer_name = layer_name.lower()\n",
    "    assert layer_name in ['cnn', 'lstm', 'lstmfcn']\n",
    "\n",
    "    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
    "                                                                      normalize_timeseries=normalize_timeseries)\n",
    "    _, sequence_length = calculate_dataset_metrics(X_train)\n",
    "\n",
    "    if sequence_length != MAX_SEQUENCE_LENGTH_LIST[dataset_id]:\n",
    "        if cutoff is None:\n",
    "            choice = cutoff_choice(dataset_id, sequence_length)\n",
    "        else:\n",
    "            assert cutoff in ['pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
    "            choice = cutoff\n",
    "\n",
    "        if choice not in ['pre', 'post']:\n",
    "            return\n",
    "        else:\n",
    "            X_train, X_test = cutoff_sequence(X_train, X_test, choice, dataset_id, sequence_length)\n",
    "\n",
    "    model.load_weights(\"./weights/%s_weights.h5\" % dataset_prefix)\n",
    "\n",
    "    conv_layers = [layer for layer in model.layers\n",
    "                   if layer.__class__.__name__ == 'Conv1D']\n",
    "\n",
    "    lstm_layers = [layer for layer in model.layers\n",
    "                   if layer.__class__.__name__ == 'LSTM' or\n",
    "                   layer.__class__.__name__ == 'AttentionLSTM']\n",
    "\n",
    "    lstmfcn_layer = model.layers[-2]\n",
    "\n",
    "    if layer_name == 'cnn':\n",
    "        feature_layer = conv_layers[-1]\n",
    "    elif layer_name == 'lstm':\n",
    "        feature_layer = lstm_layers[-1]\n",
    "    else:\n",
    "        feature_layer = lstmfcn_layer\n",
    "\n",
    "    dataset_name = os.path.split(dataset_prefix)[-1]\n",
    "\n",
    "    if dataset_name is None or len(dataset_name) == 0:\n",
    "        dataset_name = dataset_prefix\n",
    "\n",
    "    save_dir = 'layer_features/%s/' % dataset_name\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    extraction_model = Model(model.input, feature_layer.output)\n",
    "\n",
    "    train_features = extraction_model.predict(X_train, batch_size=128)\n",
    "    test_features = extraction_model.predict(X_test, batch_size=128)\n",
    "\n",
    "    shape = train_features.shape\n",
    "    if len(shape) > 2:\n",
    "        train_features = train_features.reshape((shape[0], shape[1] * shape[2]))\n",
    "\n",
    "    shape = test_features.shape\n",
    "    if len(shape) > 2:\n",
    "        test_features = test_features.reshape((shape[0], shape[1] * shape[2]))\n",
    "\n",
    "    print(\"Train feature shape : \", train_features.shape, \"Classes : \", len(np.unique(y_train)))\n",
    "    print(\"Test features shape : \", test_features.shape, \"Classes : \", len(np.unique(y_test)))\n",
    "\n",
    "    np.save(save_dir + '%s_%s_train_features.npy' % (layer_name, dataset_name), train_features)\n",
    "    np.save(save_dir + '%s_%s_train_labels.npy' % (layer_name, dataset_name), y_train)\n",
    "    np.save(save_dir + '%s_%s_test_features.npy' % (layer_name, dataset_name), test_features)\n",
    "    np.save(save_dir + '%s_%s_test_labels.npy' % (layer_name, dataset_name), y_test)\n",
    "\n",
    "    print(\"Saved train feature vectors at %s\" % (save_dir + '%s_%s_train_features.npy' % (layer_name, dataset_name)))\n",
    "    print(\"Saved test feature vectors at %s\" % (save_dir + '%s_%s_test_features.npy' % (layer_name, dataset_name)))\n",
    "    print()\n",
    "\n",
    "\n",
    "class MaskablePermute(Permute):\n",
    "\n",
    "    def __init__(self, dims, **kwargs):\n",
    "        super(MaskablePermute, self).__init__(dims, **kwargs)\n",
    "        self.supports_masking = True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
